#!/usr/bin/env -S python3 -u

import argparse, zipfile, os, sys, itertools, re, tempfile
from multiprocessing import Pool, get_context, current_process, set_start_method
from math import sqrt
from subprocess import check_output, CalledProcessError, DEVNULL
import logging
import logging.config

def lognprint(s, q, level=logging.DEBUG):
    log.log(level, s)
    if not q and level > logging.DEBUG:
        print(s, flush=True)

try:
    from usfmtc import USX, _grammarDoc, _usfmGrammar
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib'))
    from usfmtc import USX, _grammarDoc, _usfmGrammar
from usfmtc.validating.rngparser import NoParseError, Parser
from usfmtc.usxmodel import addesids, etCmp, cleanup, partypes
from usfmtc.usfmparser import Grammar
import xml.etree.ElementTree as et
from lxml import etree

def proconezip(fpath, sfmp, log, relax, args):
    lognprint(f"{fpath}: started", args.quiet)
    try:
        zipf = zipfile.ZipFile(fpath)
    except (OSError, zipfile.BadZipFile) as e:
        lognprint(f"{fpath}: Skipped {e}", args.quiet, logging.INFO)
        return (fpath, [])
    try:
        fh = zipf.open("source/source.zip")
    except (KeyError, OSError) as e:
        zipf.close()
        lognprint(f"{fpath}: Skipped (no source) {e}", args.quiet, logging.INFO)
        return (fpath, [])
    try:
        szip = zipfile.ZipFile(fh)
    except zipfile.BadZipFile as e:
        zipf.close()
        lognprint(f"{fpath}: Skipped. Bad Zip file source/source.zip {e}", args.quiet, logging.INFO)
        return (fpath, [])
    results = []
    anyfailed = False
    bookcount = 0
    try:
        if args.do in ("all", "usfm"):
            for f in szip.namelist():
                if f.lower().endswith('sfm'):
                    m = re.match(r'(\d{2})', f)
                    if m is None or not (0 < int(m.group(1)) < 68):
                        continue
                else:
                    continue
                if args.verbose:
                    lognprint(f"{fpath}/{f}", args.quiet)
                sfmfh = szip.open(f)
                sfmdat = sfmfh.read().decode("utf-8", errors="ignore")
                result = None
                failed = False
                bookcount += 1
                try:
                    result = USX.fromUsfm(sfmdat, sfmp, altparser=args.validate, timeout=args.timeout or 3600, strict=True)
                except NoParseError as e:
                    imsg = " {} at {} in {}".format(e.msg, e.state.pos, f).replace("\r", "\\r")
                    results.append(imsg)
                    lognprint(f"{fpath}/{f}: {imsg}", args.quiet)
                    failed = True
                except TimeoutError:
                    lognprint(f"{fpath}: Timed out", args.quiet, logging.INFO)
                    results.append("Timed out")
                    failed = True
                except UnicodeError:
                    lognprint(f"{fpath}: Skipped. Encoding error, probably latin-1?", args.quiet, logging.INFO)
                    results.append("Encoding error")
                    failed = True
                except SyntaxError as e:
                    lognprint(f"{fpath}/{f}: {e}", args.quiet, logging.INFO)
                    failed = True
                if result is not None:
                    result.addesids()
                    restxt = et.tostring(result.getroot(), encoding="utf-8", xml_declaration=True)
                    etreexml = etree.fromstring(restxt)
                    if not relax.validate(etreexml):
                        xmsg = str(relax.error_log.last_error)
                        lognprint(f"{fpath}/{f}: {xmsg}", args.quiet, logging.DEBUG)
                        if failed:
                            results[-1] += f" |xml>"
                        else:
                            results.append(f"{fpath}/{f} |xml>")
                        lognprint(f"{fpath}/{f}: XML| generated xml invalid", args.quiet, logging.INFO)
                        failed = True
                        if args.jing:
                            with tempfile.NamedTemporaryFile(delete=False) as outf:
                                fname = outf.name
                                outf.write(restxt)
                            try:
                                report = check_output(f"jing {args.grammar} {fname}", shell=True, stderr=DEVNULL, encoding="utf-8")
                            except CalledProcessError as e:
                                report = e.output
                            lognprint(f"{fpath}/{f}: {report}", args.quiet, logging.DEBUG)
                            os.unlink(fname)
                anyfailed |= failed
                if failed and args.oneerror:
                    break
            szip.close()
        if args.do in ("all", "usx"):
            usxjobs = [x for x in zipf.namelist() if x.startswith("release/USX_1/") and x.lower().endswith(".USX")]
            for ausx in usxjobs:
                with zipf.open(ausx) as xf:
                    doc = etree.parse(xf)
                    if not relax.validate(doc):
                        xmsg = str(relax.error_log.last_error)
                        lognprint(f"{fpath}/{ausx}: {xmsg}", args.quiet, logging.DEBUG)
                        err = f"{fpath}/{ausx} XML, USX xml invalid"
                        results.append(err)
                        lognprint(err, args.quiet, logging.INFO)
                        anyfailed = True
                        if args.oneerror:
                            break
                    bookcount += 1
        zipf.close()
        if bookcount == 0 and not anyfailed:
            results = [f"{fpath}: No files found"]
            lognprint(results[0], args.quiet, logging.INFO)
        lognprint("{}: {}".format(fpath, "Passed" if not anyfailed else "Failed"), args.quiet, logging.INFO)
        return (fpath, [] if not anyfailed else [results])
    except Exception as e:
        lognprint(f"{fpath}/{f}: {type(e)} {e}", args.quiet, logging.INFO)
        lognprint("{}: {}".format(fpath, "Crashed"), args.quiet, logging.INFO)
        return (fpath, [str(e)])


parser = argparse.ArgumentParser()
parser.add_argument("directory",help="A single or tree of test directories")
parser.add_argument("-g","--grammar",required=True,help="Enhanced usx.rng RELAXng grammar")
parser.add_argument("-D","--do",default="all",help="Do all*, usx, usfm tests")
parser.add_argument("-S","--start",default="Scripture",help="Starting node for parsing")
parser.add_argument("-E","--extfiles",action='append',default=[],help='markers.ext files to include')
parser.add_argument("-V","--validate",action="store_true",help="use validating parsers for USFM")
parser.add_argument("-s","--size",type=float,default=500000,help="Max test file to run in kB")
parser.add_argument("-M","--match",help="Constrain input files to match regex")
parser.add_argument("--skipfile",help="Logfile name and skip all jobs skipped and passed in that file")
parser.add_argument("-O","--oneerror",action="store_true",help="Bail on a zip after the first faulty sfm file")
parser.add_argument("-T","--timeout",type=int,help="Maximum time to process a file in seconds")
parser.add_argument("-J","--jing",action="store_true",default=False,help="Run jing on failed xml files")
parser.add_argument("-j","--jobs",type=int,default=0,help="Parallel jobs, single=1")
parser.add_argument("-C","--chunks",type=int,help="Size of parallel chunks")
parser.add_argument("-v","--verbose",action="store_true",help="Print things like error messages")
parser.add_argument("-q","--quiet",action="store_true",help="Don't report progress")
parser.add_argument("-l","--logging",default="error",help="Set logging level to usfmxtest.log")
parser.add_argument("--logfile",default="usfmtestdbl.log",help="Out log to file")
parser.add_argument("--logparse",action="store_true",help="Log the parsing as well")
parser.add_argument("-z","--debug",type=int,default=0,help="1=print tree, 2-alt multiproc")
args = parser.parse_args()

def chunkjobs(jobs, nchunks):
    chunksize = len(jobs) // nchunks
    chunksize += 1 if len(jobs) % nchunks else 0
    tjobs = [j[0] for j in sorted(jobs, key=lambda x:(-x[1], x[0]))]
    dojobs = []
    for i in range(nchunks):
        if args.debug & 2 == 0:
            lj = [x for x in itertools.chain(*itertools.zip_longest(
                    tjobs[i::2*nchunks], tjobs[2*nchunks-i-1::2*nchunks])) if x is not None]
        #dojobs.extend(x for x in itertools.chain(*zip(tjobs[i::2*nchunks]+[None], tjobs[2*nchunks-i-1::2*nchunks]+[None])))
        else:
            lj = tjobs[i::2*nchunks] + tjobs[2*nchunks-i-1::2*nchunks]
        dojobs.extend(lj + ([None] * (chunksize - len(lj))))
    sys.stderr.write(f"{len(dojobs)} jobs from {len(jobs)}, {chunksize=}, {nchunks=}\n".format(len(dojobs)))
    return dojobs

if args.logging:
    try:
        loglevel = int(args.logging)
    except ValueError:
        loglevel = getattr(logging, args.logging.upper(), None)
    if isinstance(loglevel, int):
        parms = {'level': loglevel, 'datefmt': '%d/%b/%Y %H:%M:%S',
                 'format': '%(asctime)s.%(msecs)03d %(levelname)s:%(module)s(%(lineno)d) %(message)s'}
        logfh = open(args.logfile, "w", encoding="utf-8")
        parms.update(stream=logfh, filemode="w") #, encoding="utf-8")
        try:
            logging.basicConfig(**parms)
        except FileNotFoundError as e:      # no write access to the log
            print("Exception", e)
    if not args.logparse:
        logging.config.dictConfig({'version': 1, 'disable_existing_loggers': True})
        if args.timeout is None:
            Parser.debug=False
    log = logging.getLogger('usfmtestdbl')

relaxns = "{http://relaxng.org/ns/structure/1.0}"

skips = {}
if args.skipfile and os.path.exists(args.skipfile):
    with open(args.skipfile, encoding="utf-8") as inf:
        for l in inf.readlines():
            m = re.match(r"^(.*): (\S+)(?:\s+\((Passed)\))?", l)
            if m is not None:
                if m.group(2) in ("Passed", "Skipped"):
                    skips[m.group(1)] = m.group(3) or m.group(2)

with open(args.grammar, "rb") as inf:
    rdoc = _grammarDoc(inf, args.extfiles, factory=etree)
if not args.validate:
    sfmproc = Grammar()
    for e in args.extfiles:
        sfmproc.readmrkrs(e)
else:
    sfmproc = _usfmGrammar(rdoc, start=args.start)

jobs = []
skipcount = 0
for dp, dns, fns in os.walk(args.directory):
    for f in fns:
        if not f.lower().endswith(".zip"):
            continue
        if args.match and not re.match(args.match, f, re.I):
            continue
        st = os.stat(os.path.join(dp, f))
        if st.st_size > args.size * 1024:
            continue
        jobname = os.path.join(dp, f)
        if jobname in skips:
            log.info(f"{jobname}: Skipped ({skips[jobname]})")
            skipcount += 1
        else:
            jobs.append((jobname, st.st_size))

if not args.quiet:
    print(f"Skipping {skipcount} jobs")

if args.jobs != 1 and len(jobs) > 1:
    set_start_method('fork')
    if args.jobs == 0:
        args.jobs = os.cpu_count()

    def initproc(newdoctxt, args, log):
        proc = current_process()
        newdoc = etree.fromstring(newdoctxt)
        rng =  etree.RelaxNG(etree=newdoc)
        proc.usxrng = rng
        if not args.validate:
            proc.sfmproc = sfmproc
        else:
            proc.sfmproc = _usfmGrammar(rdoc, start=args.start)
        proc.log = log
        proc.args = args
    def doprocone(arg):
        if arg is None:
            return
        proc = current_process()
        res = proconezip(arg, proc.sfmproc, proc.log, proc.usxrng, proc.args)
        return (str(res[0]), [str(x) for x in res[1]])
    #pool = get_context('spawn').Pool()
    newdoctxt = et.tostring(rdoc.getroot())
    pool = Pool(args.jobs or None, initializer=initproc, initargs=(newdoctxt, args, log))
    if args.chunks:
        chunksize = args.chunks
        nchunks = len(jobs) // chunksize
    else:
        nchunks = args.jobs * 4
        chunksize = len(jobs) // nchunks
    dojobs = chunkjobs(jobs, nchunks)
    mapresults = pool.map_async(doprocone, dojobs, chunksize=chunksize)
    results = mapresults.get()
else:
    usxrng = etree.RelaxNG(etree=rdoc)
    results = []
    for j in jobs:
        results.append(proconezip(j[0], sfmproc, log, usxrng, args))

failed = 0
passed = 0
skipped = 0
for r in results:
    if r is None or r[1] is None:
        skipped += 1
    elif len(r[1]) == 0:
        passed += 1
    else:
        failed += 1
        if args.verbose:
            print("{}:\n    {}".format(r[0], "    \n".join(r[1])))
total = failed + passed + skipped
print(f"{passed=}, {failed=}, {skipped=}, {total=}")

